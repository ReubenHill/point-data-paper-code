{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on this Notebook\n",
    "This can be run as either a python/ipython script or as a notebook.\n",
    "It generates a firedrake `DumbCheckpoint` file called `true-fields.h5` containing the values of $u_{true}$ and $q_{true}$ in `Function`s named `u_true` and `q_true` respectively.\n",
    "The investigation continues in another notebook which uses these fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "\n",
    "We want to find out how the solution of our inverse problem converges as we increase the number of points for both the new and traditional methods of data interpolation.\n",
    "\n",
    "If we have what is known as **\"posterior consistency\"** then we expect that the error in our solution, when compared to the true solution, will always decrease as we increase the number of points we are assimilating.\n",
    "\n",
    "## Posterior Consistency\n",
    "\n",
    "From a Bayesian point of view, the regularisation we choose and the weighting we give it encode information about our assumed prior probability distribution of $q$ before we start assimilating data (adding observations).\n",
    "Take, for example, the regularisation used in the this problem\n",
    "\n",
    "$$\n",
    "\\frac{\\alpha^2}{2}\\int_\\Omega|\\nabla q|^2dx\n",
    "$$\n",
    "\n",
    "which asserts a prior that the solution $q$ which minimises $J$ should be smooth and gives a weighting $\\alpha$ to the assertion.\n",
    "If we have posterior consistency, the contribution of increasing numbers of measurements $u_{obs}$ should increase the weighting of our data relative to our prior and we should converge towards the true solution.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Our two methods minimise two different functionals. \n",
    "The first minimises $J$\n",
    "\n",
    "$$J[u, q] = \n",
    "\\underbrace{\\frac{1}{2}\\int_{\\Omega_v}\\left(\\frac{u_{obs} - I(u, \\text{P0DG}(\\Omega_v))}{\\sigma}\\right)^2dx}_{\\text{model-data misfit}} + \n",
    "\\underbrace{\\frac{\\alpha^2}{2}\\int_\\Omega|\\nabla q|^2dx}_{\\text{regularization}}$$\n",
    "\n",
    "whilst the second minimises $J''$\n",
    "\n",
    "$$J''[u, q] = \n",
    "\\underbrace{\\frac{1}{2}\\int_{\\Omega}\\left(\\frac{u_{interpolated} - u}{\\hat{\\sigma}}\\right)^2dx}_{\\text{model-data misfit}} + \n",
    "\\underbrace{\\frac{\\alpha^2}{2}\\int_\\Omega|\\nabla q|^2dx}_{\\text{regularization}}.$$\n",
    "\n",
    "**where $\\hat{\\sigma}$ is an appropriate value for the given regularisation term $\\alpha$ found with an l-curve analysis.**\n",
    "\n",
    "As set up here increasing the number of points to assimilate has the effect of increasing the size of the misfit term in $J$ (with a weight proportional to each measurement's variance $\\sigma$) so we expect to converge to $q_{true}$ as the number of measurements increases.\n",
    "\n",
    "As we increase the number of measurements in $J''$ we hope that our calculated $u_{interpolated}$ approaches $u$ (to therefore minimise the misfit). There is, however, no mechanism to cause the misfit term to increase relative to the regularizatuion term.\n",
    "\n",
    "We therefore predict that minimising $J$ will display posterior consistency and that minimising the various $J'$ for each $u_{interpolated}$ will not.\n",
    "\n",
    "## Hypothesis Amendment! A note on finite element method error\n",
    "Note that our solutions all exist in finite element spaces which are usually approximations of a true solution with some error that (hopefully) decreases as mesh density increase and solution space order increase.\n",
    "Since I am comparing to a solution $u_true$ in CG2 space I expect, at best, that we will converge to $u_true$ when we have, on average, enough points per cell to fully specify the lagrange polynomials in that cell.\n",
    "Were we in CG1 this would be 3 points per cell (I can't remember how many we would need for CG2!) to give convergence if those measurements had no noise.\n",
    "Since our measurements are noisy I do not expect actual convergence, but I anticipate some slowing in convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import (\n",
    "    LinearNDInterpolator,\n",
    "    NearestNDInterpolator,\n",
    "    CloughTocher2DInterpolator,\n",
    "    Rbf,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import firedrake\n",
    "import firedrake_adjoint\n",
    "\n",
    "from firedrake import Constant, cos, sin\n",
    "\n",
    "import numpy as np\n",
    "from numpy import pi as π\n",
    "from numpy import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import os\n",
    "\n",
    "currentdir = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = firedrake.UnitSquareMesh(32, 32)\n",
    "\n",
    "# Solution Space\n",
    "V = firedrake.FunctionSpace(mesh, family='CG', degree=2)\n",
    "\n",
    "# q (Control) Space\n",
    "Q = firedrake.FunctionSpace(mesh, family='CG', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake $q_{true}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "generator = random.default_rng(seed)\n",
    "\n",
    "degree = 5\n",
    "x = firedrake.SpatialCoordinate(mesh)\n",
    "\n",
    "q_true = firedrake.Function(Q, name='q_true')\n",
    "for k in range(degree):\n",
    "    for l in range(int(np.sqrt(degree**2 - k**2))):\n",
    "        Z = np.sqrt(1 + k**2 + l**2)\n",
    "        ϕ = 2 * π * (k * x[0] + l * x[1])\n",
    "\n",
    "        A_kl = generator.standard_normal() / Z\n",
    "        B_kl = generator.standard_normal() / Z\n",
    "\n",
    "        expr = Constant(A_kl) * cos(ϕ) + Constant(B_kl) * sin(ϕ)\n",
    "        mode = firedrake.interpolate(expr, Q)\n",
    "\n",
    "        q_true += mode\n",
    "\n",
    "print('Made fake q_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake $u_{true}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import exp, inner, grad, dx\n",
    "u_true = firedrake.Function(V, name='u_true')\n",
    "v = firedrake.TestFunction(V)\n",
    "f = Constant(1.0)\n",
    "k0 = Constant(0.5)\n",
    "bc = firedrake.DirichletBC(V, 0, 'on_boundary')\n",
    "F = (k0 * exp(q_true) * inner(grad(u_true), grad(v)) - f * v) * dx\n",
    "firedrake.solve(F == 0, u_true, bc)\n",
    "\n",
    "print('Made fake u_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear tape since don't need to have taped above\n",
    "tape = firedrake_adjoint.get_working_tape()\n",
    "tape.clear_tape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Output\n",
    "We save our fields to a firedrake checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(currentdir, 'true-fields')\n",
    "\n",
    "with firedrake.DumbCheckpoint(filename, mode=firedrake.FILE_CREATE) as chk:\n",
    "    chk.store(q_true)\n",
    "    chk.store(u_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure they have saved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with firedrake.DumbCheckpoint(filename, mode=firedrake.FILE_READ) as chk:\n",
    "    chk.load(q_true, name='q_true')\n",
    "    chk.load(u_true, name='u_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.set_aspect('equal')\n",
    "colors = firedrake.tripcolor(q_true, axes=axes, shading='gouraud')\n",
    "fig.colorbar(colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.set_aspect('equal')\n",
    "colors = firedrake.tripcolor(u_true, axes=axes, shading='gouraud')\n",
    "fig.colorbar(colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
